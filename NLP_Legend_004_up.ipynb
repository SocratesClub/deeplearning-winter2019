{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://wiki.swarma.net/images/e/e7/集智AI学园首页左上角logo_2017.8.17.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI 有嘻哈 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最近“中国有嘻哈”特别火。\n",
    "\n",
    "我也来跟风，所以本周的 NLP 教程是拿循环神经网络（RNN）来生成嘻哈歌词。\n",
    "\n",
    "让我们看看一个简单的深度神经网络模型，能生成什么样的“嘻哈歌词”。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单来说，中文句子中的每个字在统计上是相关的。比如对于“多想你陪我透透气”这句话，如果我们知道第一个字“多”，那下一字有可能是“想”；如果知道前两个字“多想”，那么第三个字是“你”的可能性就大些，以此类推，如果知道“多想你陪我透透”，那么最后一个字很有可能就是“气”字。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://thumbsnap.com/T1pYyFE0\" title=\"Image Hosted by ThumbSnap\"><img src=\"http://thumbsnap.com/s/T1pYyFE0.jpg\" alt=\"Free Photo Sharing by ThumbSnap\" /></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以这次搭建的循环神经网络（RNN）模型，就是要在训练的过程中学习到“歌词之间的规律”。\n",
    "\n",
    "我们只要给训练好的模型一个开头，比如“我们”，模型就能按照学习到的规律，继续预测生成“我们”之后的嘻哈歌词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 清理训练数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先准备训练模型的数据。\n",
    "\n",
    "我从网上爬了大约三万四千行嘻哈歌词（想知道我怎么爬的么？请持续关注集智AI学园）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' 作曲 : Mixtape\\n', ' 作词 : 啊之\\n', '\\n', '录音 MISO MUSIC\\n', '混缩 MAI\\n', 'MIX BY MAI\\n', \"hey baby Don't worry\\n\", \"hey baby Don't worry\\n\", \"hey baby Don't worry\\n\", \"hey baby Don't worry\\n\", '抱歉我依旧不稳定 DAMN\\n', '鱼龙混杂的街头不只靠努力 REAL 别烦恼\\n', '多想你陪我透透气 发发牢骚\\n', '倾诉着最近不如意 let me let me\\n', '这就是生活里 的问题 不必不报忧\\n', '做真实的自己 不用比 早晚都能够\\n', '我始终担心你 出问题 人心难看透\\n', '我怎么都可以 唯有你 所以 所以\\n', '我早已日夜颠倒 烟酒成瘾 制作巧克力\\n']\n"
     ]
    }
   ],
   "source": [
    "lrc_lines = open('../data/rapper.txt').readlines()\n",
    "print(lrc_lines[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 去除无关信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "歌词中包含“作词”，“作曲”等信息，这对训练模型生成歌词是没有任何帮助的，先去掉他们。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 去除文本中不是属于歌词的行\n",
    "# 我的程序写的都比较水啦\n",
    "# 大家可以自由发挥写出更好的程序\n",
    "for i in range(len(lrc_lines)):\n",
    "    line = lrc_lines[i]\n",
    "    if \"作词\" in line or \"作曲\" in line or \"编曲\" in line or \"录音\" in line or \"混缩\" in line or \"制作人\" in line:\n",
    "        lrc_lines[i] = \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "观察下已经去除作者信息的歌词文本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', '\\n', '\\n', '\\n', '\\n', '\\n', 'MIX BY MAI\\n', \"hey baby Don't worry\\n\", \"hey baby Don't worry\\n\", \"hey baby Don't worry\\n\", \"hey baby Don't worry\\n\", '抱歉我依旧不稳定 DAMN\\n', '鱼龙混杂的街头不只靠努力 REAL 别烦恼\\n', '多想你陪我透透气 发发牢骚\\n', '倾诉着最近不如意 let me let me\\n', '这就是生活里 的问题 不必不报忧\\n', '做真实的自己 不用比 早晚都能够\\n', '我始终担心你 出问题 人心难看透\\n', '我怎么都可以 唯有你 所以 所以\\n', '我早已日夜颠倒 烟酒成瘾 制作巧克力\\n']\n"
     ]
    }
   ],
   "source": [
    "print(lrc_lines[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到现在歌词中还包括一些英文，这是嘻哈的特色。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 减少语言种类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练数据中存在的语言种类越多，意味着深度神经网络模型学习起来的难度越大。\n",
    "\n",
    "因为这次做的是一个简单的模型，所以我不打算让模型再去预测英文歌词，所以我要把歌词中的英文去掉。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['抱歉我依旧不稳定\\n', '鱼龙混杂的街头不只靠努力\\n', '别烦恼\\n', '多想你陪我透透气\\n', '发发牢骚\\n', '倾诉着最近不如意\\n', '这就是生活里\\n', '的问题\\n', '不必不报忧\\n', '做真实的自己\\n']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "final_lrc = []\n",
    "# 使用正则表达式的手段识别中文\n",
    "# 中文（包括繁体）在unicode中的编码范围是u4e00~u9fa5\n",
    "zhPattern = re.compile(u'[\\u4e00-\\u9fa5]+')\n",
    "\n",
    "# 遍历所有歌词行\n",
    "for line in lrc_lines:\n",
    "    # findall会返回一个列表\n",
    "    zh_list = zhPattern.findall(line)\n",
    "    # 将中文拷贝到 final_lrc 中去\n",
    "    for item in zh_list:\n",
    "        final_lrc.append(item + '\\n')\n",
    "\n",
    "# 这样就只剩中文啦！\n",
    "print(final_lrc[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立“字典”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还是像之前一样对所有文字建立一个“文字表”，因为其中元素都是独立的汉字，所以这次我们称它为“字典”。\n",
    "\n",
    "在字典中所有的文字都会有对应的一个索引号。\n",
    "\n",
    "在训练神经网络模型的时候，传入神经网络的是某个字的“索引号”，而不是这个字本身。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "礼 : 848\n",
      "嘞 : 3243\n",
      "惬 : 212\n",
      "歪 : 3160\n",
      "以 : 73\n",
      "款 : 782\n",
      "遍 : 1182\n",
      "伸 : 2091\n",
      "體 : 2541\n",
      "獠 : 3167\n"
     ]
    }
   ],
   "source": [
    "# 字典\n",
    "word_to_ix = {}\n",
    "\n",
    "for line in final_lrc:\n",
    "    for word in line:\n",
    "        if word not in word_to_ix:\n",
    "            # 在单词表的末端添加这个单词\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "            \n",
    "# 打印出字典中的前10个元素，注意它是无序的\n",
    "for i, key in zip(range(10), word_to_ix):\n",
    "    print(key, \":\", word_to_ix[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了“字典”，还需要建立一个“有序列表”。\n",
    "\n",
    "“有序列表”就是将“字典”按照索引号从小到大的顺序排序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['抱', '歉', '我', '依', '旧', '不', '稳', '定', '\\n', '鱼']\n",
      "抱 : 0\n",
      "歉 : 1\n",
      "我 : 2\n",
      "依 : 3\n",
      "旧 : 4\n",
      "不 : 5\n",
      "稳 : 6\n",
      "定 : 7\n",
      "\n",
      " : 8\n",
      "鱼 : 9\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "# 排序\n",
    "sorted_char_list = sorted(word_to_ix.items(), key=operator.itemgetter(1), reverse=False) \n",
    "\n",
    "# 有序列表\n",
    "# 这里注意字典是“dict”类型\n",
    "# 有序列表是“list”类型\n",
    "char_list = []\n",
    "for item in sorted_char_list:\n",
    "    char_list.append(item[0])\n",
    "\n",
    "# 有序列表中的元素\n",
    "print(char_list[:10])\n",
    "\n",
    "# 观察有序列表中的元素即字典的有序排列\n",
    "for ch in char_list[:10]:\n",
    "    print(ch, \":\", word_to_ix[ch])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“字典”的长度代表训练数据中有多少不同的字，这个长度即是我们模型输入层的大小，我们建立一个变量 `nn_characters` 来保存它。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3801\n"
     ]
    }
   ],
   "source": [
    "n_characters = len(word_to_ix)\n",
    "print(n_characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练前的准备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据随机选择器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这次搭建的神经网络模型是“字符级”的，即每次输入“一个字”。\n",
    "\n",
    "所以先要把上面处理过后的“一条条”的数据，都保存到一个“大字符串”里，便于随机取用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "抱歉我依旧不稳定\n",
      "鱼龙混杂的街头不只靠努\n"
     ]
    }
   ],
   "source": [
    "big_string = ''\n",
    "for line in final_lrc:\n",
    "    for ch in line:\n",
    "        big_string += ch\n",
    "        \n",
    "print(big_string[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后我们再编写一个工具方法“random_chunk”，用于每次从训练数据中随机选择201个字。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "chunk_len = 200\n",
    "\n",
    "# 随机选取200 + 1个字符的数据\n",
    "def random_chunk():\n",
    "    # 起点的可选范围：0 ~ final_lrc_len-chunk_len\n",
    "    start_index = random.randrange(0, len(big_string) - chunk_len)\n",
    "    end_index = start_index + chunk_len + 1\n",
    "    # 将抽取的列表项转化为字符串再返回\n",
    "    return big_string[start_index:end_index]\n",
    "\n",
    "result = random_chunk()\n",
    "print(len(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意这里选取的字符串长度实际为201，为什么是201，到下面建立输入和目标数据的时候你就知道。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将数据转化为张量（Tensor）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "建立一个辅助方法“char_tensor”，用于将文字转化为张量（Tensor）。\n",
    "\n",
    "张量中保存的是文字对应的索引。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201\n",
      "Variable containing:\n",
      "  200\n",
      "  189\n",
      " 1006\n",
      " 1458\n",
      "    8\n",
      "  395\n",
      "  426\n",
      "  119\n",
      "  195\n",
      "  173\n",
      "[torch.LongTensor of size 10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def char_tensor(string):\n",
    "    # 先按指定长度创建一个longTensor，填充0\n",
    "    tensor = torch.zeros(len(string)).long()\n",
    "    # 逐字查找字典\n",
    "    # 取出每个字的索引号，保存到 Tensor 中\n",
    "    for c in range(len(string)):\n",
    "        tensor[c] = word_to_ix[string[c]]\n",
    "    return Variable(tensor)\n",
    "\n",
    "# 来试试这个方法\n",
    "index_tensor = char_tensor(random_chunk())\n",
    "\n",
    "# 查看 Tensor 长度\n",
    "print(len(index_tensor))\n",
    "# 查看 Tensor 中保存的索引\n",
    "print(index_tensor[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输入和目标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面我们建立了“random_chunk”，用于选取201条数据。\n",
    "\n",
    "为什么是201条？让我们回头看看文章开头的那张图：\n",
    "\n",
    "<a href=\"http://thumbsnap.com/T1pYyFE0\" title=\"Image Hosted by ThumbSnap\"><img src=\"http://thumbsnap.com/s/T1pYyFE0.jpg\" alt=\"Free Photo Sharing by ThumbSnap\" /></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN 模型在输入“多”时，需要预测出“想”；在输入“想”时，需要预测“你”。\n",
    "\n",
    "即如果我们随机选取的总数据是“多想你陪我透透气”，那么训练输入数据是“多想你陪我”，训练目标数据是“想你陪我透透气”。\n",
    "\n",
    "训练输入数据与训练目标数据正好错开一个字，那么当我们选取了201条数据，那输入数据和目标数据就正好是各200条。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "def random_training_set():    \n",
    "    # 先选取201字符长度的文本\n",
    "    chunk = random_chunk()\n",
    "    # 将选取的文本全部转化为 Tensor\n",
    "    inp = char_tensor(chunk[:-1])\n",
    "    target = char_tensor(chunk[1:])\n",
    "    return inp, target\n",
    "\n",
    "inp, target = random_training_set()\n",
    "print(len(inp))\n",
    "print(len(target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "到这里我们在训练前要做的准备就全部完成了，下面要做的就是搭建模型、训练模型、评估模型了。\n",
    "\n",
    "为了控制篇幅长度，对模型操作的内容我都放在“终篇”中了。不出意外的话，“终篇”将在周五晚间放出，敬请期待！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://wiki.swarma.net/images/c/ca/AI学园.jpg)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
